# Attention-Is-All-You-Need---note

There is a note for reading the paper([Attention Is All You Need])

[Attention Is All You Need]: https://arxiv.org/abs/1706.03762

# Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
There is a note for reaing the paper ([Swin Transformer: Hierarchical Vision Transformer using Shifted Windows])
[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows]:https://arxiv.org/abs/2103.14030

